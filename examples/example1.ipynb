{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例1：快速事件相关设计中的单审GLM估计\n",
    "自然场景数据集核心实验，subj01，nsd01扫描会议\n",
    "GLMsingle是一个新的工具，提供高效、可扩展和准确的单审fMRI反应估计。\n",
    "\t这个例子1笔记本的目的是引导用户通过对GLMsingle的基本调用，使用一个有代表性的、小规模的测试数据集（在这种情况下，一个快速事件相关视觉fMRI数据集的例子会话--自然场景数据集核心实验）。\n",
    "\n",
    "\t其目的是检查GLMsingle对单次试验fMRI反应估计的可靠性的影响。默认情况下，该工具实现了一组优化，通过以下方式改进通用的GLM方法。\n",
    "\t\t(1) 确定每个体素的最佳血流动力学反应函数（HRF），\n",
    "\t\t(2) 通过 \"GLMdenoise \"得出一组有用的GLM滋扰回归因子，并挑选一个最佳数量包含在最终的GLM中，以及\n",
    "\t\t(3) 使用称为 \"fracridge \"的有效技术在每个体素应用自定义数量的山脊正规化。GLMsingle的输出是GLM betas，反映了每个体素在响应每个实验刺激或条件时的估计信号变化百分比。\n",
    "\n",
    "\t除了直接提高对重复刺激的神经反应的可靠性，这些优化的信号估计技术可以产生一系列理想的下游效应，如：提高数据集内和数据集之间的跨主体代表性相似性；通过MVPA提高诱发神经模式的单图像解码能力；以及在分析fMRI GLM输出时减少在邻近时间点观察到的空间模式的相关性。请看我们在V-VSS 2020上的视频演示，以了解在最近的大规模fMRI数据集（自然场景数据集和BOLD5000）中观察到的这些现象的摘要：https://www.youtube.com/watch?v=yb3Nn7Han8o\n",
    "\n",
    "例子1包含了一个完整的过程，即加载一个示例数据集和设计矩阵，使用GLMsingle估计神经反应，估计每个体素的反应的可靠性，并将通过GLMsingle实现的反应与使用基线GLM实现的反应进行比较。在加载和可视化格式化的fMRI时间序列及其相应的设计矩阵后，我们将描述GLMsingle的默认行为，并展示如何根据用户的需要修改超参数。在整个笔记本中，我们将使用数字、打印语句和注释来强调重要的指标和输出。\n",
    "如果用户遇到错误、意外输出或其他有关GLMsingle的问题，请毫不犹豫地在GitHub上提出问题：https://github.com/kendrickkay/GLMsingle/issues\n",
    "\n",
    "# Example 1: single-trial GLM estimation in a rapid event-related design\n",
    "\n",
    "### Natural Scenes Dataset core experiment, subj01, nsd01 scan session\n",
    "\n",
    "---------------------\n",
    "\n",
    "##### GLMsingle is new tool that provides efficient, scalable, and accurate single-trial fMRI response estimates.\n",
    "\n",
    "The purpose of this Example 1 notebook is to guide the user through basic\n",
    "calls to GLMsingle, using a representative, small-scale test dataset (in\n",
    "this case, an example session from a rapid event-related visual fMRI\n",
    "dataset - the Natural Scenes Dataset core experiment).\n",
    "\n",
    "The goal is to examine the effect of GLMsingle on the reliability of\n",
    "single-trial fMRI response estimates. By default, the tool implements a\n",
    "set of optimizations that improve upon generic GLM approaches by: (1)\n",
    "identifying an optimal hemodynamic response function (HRF) at each voxel,\n",
    "(2) deriving a set of useful GLM nuisance regressors via \"GLMdenoise\" and\n",
    "picking an optimal number to include in the final GLM, and (3) applying a\n",
    "custom amount of ridge regularization at each voxel using an efficient\n",
    "technique called \"fracridge\". The output of GLMsingle are GLM betas\n",
    "reflecting the estimated percent signal change in each voxel in response\n",
    "to each experimental stimulus or condition being modeled.\n",
    "\n",
    "Beyond directly improving the reliability of neural responses to repeated\n",
    "stimuli, these optimized techniques for signal estimation can have a\n",
    "range of desirable downstream effects such as: improving cross-subject\n",
    "representational similarity within and between datasets; improving the\n",
    "single-image decodability of evoked neural patterns via MVPA; and,\n",
    "decreasing the correlation in spatial patterns observed at neighboring\n",
    "timepoints in analysis of fMRI GLM outputs. See our video presentation at\n",
    "V-VSS 2020 for a summary of these phenomena as observed in recent\n",
    "massive-scale fMRI datasets (the Natural Scenes Dataset and BOLD5000):\n",
    "https://www.youtube.com/watch?v=yb3Nn7Han8o\n",
    "\n",
    "**Example 1 contains a full walkthrough of the process of loading an\n",
    "example dataset and design matrix, estimating neural responses using\n",
    "GLMsingle, estimating the reliability of responses at each voxel, and\n",
    "comparing those achieved via GLMsingle to those achieved using a baseline\n",
    "GLM.** After loading and visualizing formatted fMRI time-series and their\n",
    "corresponding design matrices, we will describe the default behavior of\n",
    "GLMsingle and show how to modify hyperparameters if the user desires.\n",
    "Throughout the notebook we will highlight important metrics and outputs\n",
    "using figures, print statements, and comments.\n",
    "\n",
    "Users encountering bugs, unexpected outputs, or other issues regarding\n",
    "GLMsingle shouldn't hesitate to raise an issue on GitHub:\n",
    "https://github.com/kendrickkay/GLMsingle/issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import function libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join, exists, split\n",
    "import time\n",
    "import urllib.request\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glmsingle.glmsingle import GLM_single\n",
    "\n",
    "# 注意：运行这段代码还需要fracridge资源库。 note: the fracridge repository is also necessary to run this code\n",
    "# for example, you could do:\n",
    "#      git clone https://github.com/nrdg/fracridge.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置路径并下载示例数据集 Set paths and download the example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取安装GLMsingle的目录的路径 get path to the directory to which GLMsingle was installed\n",
    "homedir = split(os.getcwd())[0]\n",
    "\n",
    "# 创建保存数据的目录 create directory for saving data\n",
    "datadir = join(homedir,'examples','data')\n",
    "os.makedirs(datadir,exist_ok=True)\n",
    "\n",
    "# 创建目录以保存实例1的输出 create directory for saving outputs from example 1\n",
    "outputdir = join(homedir,'examples','example1outputs')\n",
    "\n",
    "print(f'directory to save example dataset:\\n\\t{datadir}\\n')\n",
    "print(f'directory to save example1 outputs:\\n\\t{outputdir}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从GLMsingle OSF库下载示例数据集 download example dataset from GLMsingle OSF repository\n",
    "# 数据来自NSD数据集（subj01, nsd01扫描会话） data comes from the NSD dataset (subj01, nsd01 scan session).\n",
    "# see: https://www.biorxiv.org/content/10.1101/2021.02.22.432340v1.full.pdf\n",
    "\n",
    "datafn = join(datadir,'nsdcoreexampledataset.mat')\n",
    "\n",
    "# 为了节省时间，如果示例数据集已经存在于磁盘上，我们将跳过下载。 to save time, we'll skip the download if the example dataset already exists on disk\n",
    "if not exists(datafn):\n",
    "    \n",
    "    print(f'Downloading example dataset and saving to:\\n{datafn}')\n",
    "    \n",
    "    dataurl = 'https://osf.io/k89b2/download'\n",
    "    \n",
    "    # 下载.mat文件到指定目录 download the .mat file to the specified directory\n",
    "    urllib.request.urlretrieve(dataurl, datafn)\n",
    "    \n",
    "# 加载包含示例数据集的结构  load struct containing example dataset\n",
    "X = sio.loadmat(datafn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组织BOLD数据，设计矩阵，元数据 Organize BOLD data, design matrices, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含每次运行中的Bold时间序列和设计矩阵的变量。  variables that will contain bold time-series and design matrices from each run\n",
    "data = []\n",
    "design = []\n",
    "\n",
    "# 遍历每一次数据的运行  iterate through each run of data\n",
    "for r in range(len(X['data'][0])):\n",
    "    \n",
    "    # 索引到结构中，将每个运行的时间序列数据追加到列表中 index into struct, append each run's timeseries data to list\n",
    "    data.append(X['data'][0,r])\n",
    "    \n",
    "    # 将每个运行设计矩阵从稀疏数组转换为完整的numpy数组，并附加上  convert each run design matrix from sparse array to full numpy array, append\n",
    "    design.append(scipy.sparse.csr_matrix.toarray(X['design'][0,r]))\n",
    "    \n",
    "# 为方便起见，获得数据卷的形状（XYZ）。 get shape of data volume (XYZ) for convenience\n",
    "xyz = data[0].shape[:3]\n",
    "xyzt = data[0].shape\n",
    "\n",
    "# 获得关于刺激持续时间和TR的元数据  get metadata about stimulus duration and TR\n",
    "stimdur = X['stimdur'][0][0]\n",
    "tr = X['tr'][0][0]\n",
    "\n",
    "# 获得识别枕叶皮层的视觉ROI掩码  get visual ROI mask identifying occipital cortex\n",
    "roi = X['ROI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化样本数据和设计矩阵  Visualize sample data and design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据 -> 由几个运行的4D体积文件（x,y,z,t）组成，其中（t）ime是第四维。在这个例子中，数据只包括一个单片，并以TR=1s准备。\n",
    "# 它是一个二进制矩阵，其中（x,y,z）=1对应于对NSD项目中使用的视觉刺激有反应的皮质区域。\n",
    "# 设计 -> 每一次运行都有一个相应的设计矩阵，每一列描述一个单一的条件（条件在不同的运行中重复）。\n",
    "# 在这个NSD扫描环节中，总共有750次试验，其中总共显示了583个不同的图像。(在下面的设计矩阵中，有583个预测列/条件，每个不同的图像一个。\n",
    "\n",
    "# data -> consists of several runs of 4D volume files (x,y,z,t)  where\n",
    "# (t)ime is the 4th dimention. in this example, data consists of only a\n",
    "# single slice and has been prepared with a TR = 1s\n",
    "\n",
    "# ROI -> manually defined region in the occipital cortex. it is a binary\n",
    "# matrix where (x,y,z) = 1 corresponds to the cortical area that responded\n",
    "# to visual stimuli used in the NSD project.\n",
    "\n",
    "# design -> each run has a corresponding design matrix where each column\n",
    "# describes a single condition (conditions are repeated across runs). each\n",
    "# design matrix is binary with 1 specfing the time (TR) when the stimulus\n",
    "# is presented on the screen.\n",
    "\n",
    "# in this NSD scan session, there are a total of 750 trials, in which a\n",
    "# total of 583 distinct images are shown. (thus, some images were presented\n",
    "# more than once.) in the design matrix plotted below, there are 583 predictor\n",
    "# columns/conditions, one per distinct image. notice that the ordering of \n",
    "# conditions is pseudo-randomized. note also that in some runs not all images \n",
    "# are shown; if a column is empty it simply means that this image is shown \n",
    "# in a different run within the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example slice from run 1\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data[0][:,:,0,0])\n",
    "plt.title('example slice from run 1',fontsize=16)\n",
    "plt.subplot(122)\n",
    "plt.imshow(data[11][:,:,0,0])\n",
    "plt.title('example slice from run 12',fontsize=16)\n",
    "\n",
    "# plot example design matrix from run 1\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(design[0],interpolation='none')\n",
    "plt.title('example design matrix from run 1',fontsize=16)\n",
    "plt.xlabel('conditions',fontsize=16)\n",
    "plt.ylabel('time (TR)',fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some relevant metadata\n",
    "print(f'There are {len(data)} runs in total\\n')\n",
    "print(f'N = {data[0].shape[3]} TRs per run\\n')\n",
    "print(f'The dimensions of the data for each run are: {data[0].shape}\\n')\n",
    "print(f'The stimulus duration is {stimdur} seconds\\n')\n",
    "print(f'XYZ dimensionality is: {data[0].shape[:3]} (one slice only in this example)\\n')\n",
    "print(f'Numeric precision of data is: {type(data[0][0,0,0,0])}\\n')\n",
    "print(f'There are {np.sum(roi)} voxels in the included visual ROI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用默认参数运行GLMsingle来估计单次试验的betas。 Run GLMsingle with default parameters to estimate single-trial betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出和数字将保存在一个文件夹中（你可以指定它的名字作为GLMsingle的第5个输出）。模型估计值也可以保存在'results'变量中，这是GLMsingle的唯一输出。\n",
    "# 下面的可选参数可以分配给一个结构，即opt = dict('wantlibrary':1, 'wantglmdenoise':1)；选项是GLMsingle的第六个输入。\n",
    "# 有许多选项可以被指定；在这里，我们对人们可能想要修改/设置的主要选项进行评论。\n",
    "# wantlibrary = 1 -- 对每个体素进行HRF拟合 wantglmdenoise = 1 -- 使用GLMdenoise wantfracridge = 1 -- 使用ridge回归来改善β估计 chunklen = 50000 -- 是我们将同时处理的体素数，对于内存较小的设置，你可能需要减少这个数字。\n",
    "# wantmemoryoutputs是一个逻辑向量[A B C D]，表示要在输出<results>中返回四种模型类型。[0 0 0 1]这意味着只返回最终的D型模型。\n",
    "# wantfileoutputs是一个逻辑向量[A B C D]，表示将四种模型类型中的哪一种保存到磁盘（假设它们被计算出来）。A = 0/1用于保存ONOFF模型的结果，B = 0/1用于保存FITHRF模型的结果，C = 0/1用于保存FITHRF_GLMdenoise模型的结果，D = 0/1用于保存FITHRF_GLMdenoise_RR模型的结果。[1 1 1 1] 表示将所有计算结果保存到磁盘。\n",
    "# numpcstotry（可选）是一个非负整数，表示进入模型的GLMdenoise PC的最大数量。 默认：10。\n",
    "# fracs（可选）是一个大于0且小于等于1的分数向量。我们会自动按降序排序，并确保分数是唯一的。这些分数表示使用分数岭回归（fracridge）和交叉验证评估的正则化水平。默认：fliplr(.05:.05:1)。一个特殊情况是当<fracs>被指定为一个标量值时。在这种情况下，交叉验证不会对D型模型进行，而我们会盲目地使用所提供的D型模型的分数值。\n",
    "\n",
    "\n",
    "# outputs and figures will be stored in a folder (you can specify its name\n",
    "# as the 5th output to GLMsingle). model estimates can be also\n",
    "# saved to the 'results' variable which is the only output of\n",
    "# GLMsingle.\n",
    "\n",
    "# optional parameters below can be assigned to a structure, i.e., opt =\n",
    "# dict('wantlibrary':1, 'wantglmdenoise':1); options are the 6th input to\n",
    "# GLMsingle.\n",
    "\n",
    "# there are many options that can be specified; here, we comment on the\n",
    "# main options that one might want to modify/set. defaults for the options\n",
    "# are indicated below.\n",
    "\n",
    "# wantlibrary = 1 -> fit HRF to each voxel \n",
    "# wantglmdenoise = 1 -> use GLMdenoise \n",
    "# wantfracridge = 1 -> use ridge regression to improve beta estimates \n",
    "# chunklen = 50000 -> is the number of voxels that we will\n",
    "#    process at the same time. for setups with lower memory, you may need to \n",
    "#    decrease this number.\n",
    "\n",
    "# wantmemoryoutputs is a logical vector [A B C D] indicating which of the\n",
    "#     four model types to return in the output <results>. the user must be\n",
    "#     careful with this, as large datasets can require a lot of RAM. if you\n",
    "#     do not request the various model types, they will be cleared from\n",
    "#     memory (but still potentially saved to disk). default: [0 0 0 1]\n",
    "#     which means return only the final type-D model.\n",
    "\n",
    "# wantfileoutputs is a logical vector [A B C D] indicating which of the\n",
    "#     four model types to save to disk (assuming that they are computed). A\n",
    "#     = 0/1 for saving the results of the ONOFF model, B = 0/1 for saving\n",
    "#     the results of the FITHRF model, C = 0/1 for saving the results of the\n",
    "#     FITHRF_GLMdenoise model, D = 0/1 for saving the results of the\n",
    "#     FITHRF_GLMdenoise_RR model. default: [1 1 1 1] which means save all\n",
    "#     computed results to disk.\n",
    "\n",
    "# numpcstotry (optional) is a non-negative integer indicating the maximum\n",
    "#     number of GLMdenoise PCs to enter into the model. default: 10.\n",
    "\n",
    "# fracs (optional) is a vector of fractions that are greater than 0\n",
    "#     and less than or equal to 1. we automatically sort in descending\n",
    "#     order and ensure the fractions are unique. these fractions indicate\n",
    "#     the regularization levels to evaluate using fractional ridge\n",
    "#     regression (fracridge) and cross-validation. default:\n",
    "#     fliplr(.05:.05:1). a special case is when <fracs> is specified as a\n",
    "#     single scalar value. in this case, cross-validation is NOT performed\n",
    "#     for the type-D model, and we instead blindly use the supplied\n",
    "#     fractional value for the type-D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个用于保存GLMsingle输出的目录  create a directory for saving GLMsingle outputs\n",
    "outputdir_glmsingle = join(homedir,'examples','example1outputs','GLMsingle')\n",
    "\n",
    "opt = dict()\n",
    "\n",
    "# 为完整性设置重要的字段（但这些字段在默认情况下会被启用）。  set important fields for completeness (but these would be enabled by default)\n",
    "opt['wantlibrary'] = 1\n",
    "opt['wantglmdenoise'] = 1\n",
    "opt['wantfracridge'] = 1\n",
    "\n",
    "# 在本例中，我们将在内存中保留相关的输出，同时也将它们保存在磁盘上。  for the purpose of this example we will keep the relevant outputs in memory and also save them to the disk\n",
    "opt['wantfileoutputs'] = [1,1,1,1]\n",
    "opt['wantmemoryoutputs'] = [1,1,1,1]\n",
    "\n",
    "# 运行python GLMsingle需要创建一个GLM_single对象，然后使用.fit()例程运行程序。  running python GLMsingle involves creating a GLM_single object and then running the procedure using the .fit() routine\n",
    "glmsingle_obj = GLM_single(opt)\n",
    "\n",
    "# 将所有超参数可视化  visualize all the hyperparameters\n",
    "pprint(glmsingle_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个例子将输出文件保存到 \"example1outputs/GLMsingle \"文件夹中，如果这些输出文件还不存在，我们将执行耗时的GLMsingle调用；否则，我们将直接从磁盘加载。\n",
    "# this example saves output files to the folder  \"example1outputs/GLMsingle\" if these outputs don't already exist, we will perform the time-consuming call to GLMsingle; otherwise, we will just load from disk.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if not exists(outputdir_glmsingle):\n",
    "\n",
    "    print(f'running GLMsingle...')\n",
    "    \n",
    "    # 运行GLMsingle。 run GLMsingle\n",
    "    results_glmsingle = glmsingle_obj.fit(\n",
    "       design,\n",
    "       data,\n",
    "       stimdur,\n",
    "       tr,\n",
    "       outputdir=outputdir_glmsingle)\n",
    "\n",
    "    # 我们将GLMsingle的输出分配给 \"results_glmsingle \"变量。注意results_glmsingle['typea']包含来自ONOFF模型的GLM估计值，其中所有图像被视为相同条件。\n",
    "    # we assign outputs of GLMsingle to the \"results_glmsingle\" variable. note that results_glmsingle['typea'] contains GLM estimates from an ONOFF model, where all images are treated as the same condition. these estimates could be potentially used to find cortical areas that respond to visual stimuli. we want to compare beta weights between conditions therefore we are not going to include the ONOFF betas in any analyses of voxel reliability\n",
    "    \n",
    "else:\n",
    "    print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_glmsingle}')\n",
    "    \n",
    "    # load existing file outputs if they exist\n",
    "    results_glmsingle = dict()\n",
    "    results_glmsingle['typea'] = np.load(join(outputdir_glmsingle,'TYPEA_ONOFF.npy'),allow_pickle=True).item()\n",
    "    results_glmsingle['typeb'] = np.load(join(outputdir_glmsingle,'TYPEB_FITHRF.npy'),allow_pickle=True).item()\n",
    "    results_glmsingle['typec'] = np.load(join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.npy'),allow_pickle=True).item()\n",
    "    results_glmsingle['typed'] = np.load(join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npy'),allow_pickle=True).item()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    '\\telapsed time: ',\n",
    "    f'{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要产出的总结 Summary of important outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLMsingle的输出在它的标题里有正式的记录。这里，我们强调几个比较重要的输出。\n",
    "# R2 -> 是以R^2（百分比）表示的模型准确性。\n",
    "# betasmd -> 是全套的单次试验β权重（X x Y x Z x TRIALS）。\n",
    "# HRFindex -> 是最佳拟合HRF的1-指数。HRFs可以用getcanonicalHRFlibrary(stimdur,tr)恢复。\n",
    "# FRACvalue --> 是为每个体素选择的分数岭回归正则化水平。接近1的值意味着正则化程度较低。\n",
    "\n",
    "\n",
    "# the outputs of GLMsingle are formally documented in its header. here, we highlight a few of the more important outputs:\n",
    "#\n",
    "# R2 -> is model accuracy expressed in terms of R^2 (percentage).\n",
    "#\n",
    "# betasmd -> is the full set of single-trial beta weights (X x Y x Z x TRIALS). beta weights are arranged in chronological order.\n",
    "#\n",
    "# HRFindex -> is the 1-index of the best fit HRF. HRFs can be recovered with getcanonicalHRFlibrary(stimdur,tr)\n",
    "#\n",
    "# FRACvalue -> is the fractional ridge regression regularization level chosen for each voxel. values closer to 1 mean less regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制一个显示GLMsingle输出的大脑切片 Plot a slice of brain showing GLMsingle outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们将绘制FIT_HRF_GLMdenoise_RR GLM的几个输出，它包含了GLMsingle的全套优化。 we are going to plot several outputs from the FIT_HRF_GLMdenoise_RR GLM, which contains the full set of GLMsingle optimizations.\n",
    "\n",
    "# 我们将绘制betas、R2、最佳HRF指数和体素frac值。  we will plot betas, R2, optimal HRF indices, and the voxel frac values\n",
    "plot_fields = ['betasmd','R2','HRFindex','FRACvalue']\n",
    "colormaps = ['RdBu_r','hot','jet','copper']\n",
    "clims = [[-5,5],[0,85],[0,20],[0,1]]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i in range(len(plot_fields)):\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    \n",
    "    if i == 0:\n",
    "        # 当绘制betas时，为简单起见，只需在所有图像演示中取平均值即可。这将产生一个关于体素在响应实验刺激时是否倾向于增加或减少其活动的总结（类似于ONOFF GLM的输出）。\n",
    "        # when plotting betas, for simplicity just average across all image presentations. this will yield a summary of whether voxels tend to increase or decrease their activity in response to the experimental stimuli (similar to outputs from an ONOFF GLM)\n",
    "        plot_data = np.nanmean(np.squeeze(results_glmsingle['typed'][plot_fields[i]]),2)\n",
    "        titlestr = 'average GLM betas (750 stimuli)'\n",
    "    \n",
    "    else:\n",
    "        # 绘制GLMsingle输出的所有其他体素指标。 plot all other voxel-wise metrics as outputted from GLMsingle\n",
    "        plot_data = np.squeeze(results_glmsingle['typed'][plot_fields[i]].reshape(xyz))\n",
    "        titlestr = plot_fields[i]\n",
    "    \n",
    "    plt.imshow(plot_data,cmap=colormaps[i],clim=clims[i])\n",
    "    plt.colorbar()\n",
    "    plt.title(titlestr)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行一个基线GLM，与GLMsingle输出进行比较 Run a baseline GLM to compare with GLMsingle outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了进行比较，我们将运行一个没有HRF拟合、GLMdenoise或脊回归正则化的标准GLM。我们将使用这个基线GLM计算每个体素的分半可靠性，然后使用GLMsingle的输出betas评估可靠性是否提高。\n",
    "# for comparison purposes we are going to run a standard GLM without HRF fitting, GLMdenoise, or ridge regression regularization. we will compute the split-half reliability at each voxel using this baseline GLM, and then assess whether reliability improves using the output betas from GLMsingle.\n",
    "\n",
    "# 基线GLM的输出目录  output directory for baseline GLM\n",
    "outputdir_baseline = join(outputdir,'GLMbaseline')\n",
    "\n",
    "# 我们将通过改变GLMsingle中 \"opt \"结构中的默认设置来运行这个基线GLM。  we will run this baseline GLM by changing the default settings in GLMsingle contained within the \"opt\" structure.\n",
    "opt = dict() \n",
    "\n",
    "# 关闭优化。 turn off optimizations\n",
    "opt['wantlibrary'] = 0 # 关闭血液动力学反应函数拟合。 switch off HRF fitting\n",
    "opt['wantglmdenoise'] = 0 # 关闭GLM去噪。 switch off GLMdenoise\n",
    "opt['wantfracridge'] = 0 # 关闭脊回归。 switch off ridge regression\n",
    "\n",
    "# 在这个例子中，我们将在内存中保留相关的输出，同时将它们保存在磁盘上......\n",
    "# 前两个指数是ON-OFF GLM和基线单次试验GLM。\n",
    "# 不需要保存第三个（+GLMdenoise）和第四个（+fracridge）输出，因为它们甚至不会被计算出来\n",
    "# for the purpose of this example we will keep the relevant outputs in memory and also save them to the disk...\n",
    "# the first two indices are the ON-OFF GLM and the baseline single-trial GLM. \n",
    "# no need to save the third (+ GLMdenoise) and fourth (+ fracridge) outputs since they will not even be computed\n",
    "opt['wantmemoryoutputs'] = [1,1,0,0] \n",
    "opt['wantfileoutputs'] = [1,1,0,0]\n",
    "\n",
    "# 运行python GLMsingle需要创建一个GLM_single对象，然后使用.fit()例程运行程序。\n",
    "# running python GLMsingle involves creating a GLM_single object and then running the procedure using the .fit() routine\n",
    "glmbaseline_obj = GLM_single(opt)\n",
    "\n",
    "# 可视化超参数，包括修改后的基线选项。 visualize the hyperparameters, including the modified baseline opts\n",
    "pprint(glmbaseline_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# if these outputs don't already exist, we will perform the call to\n",
    "# GLMsingle; otherwise, we will just load from disk.\n",
    "if not exists(outputdir_baseline):\n",
    "    \n",
    "    print(f'running GLMsingle...')\n",
    "\n",
    "    # run GLMsingle, fitting the baseline GLM\n",
    "    results_assumehrf = glmbaseline_obj.fit(\n",
    "       design,\n",
    "       data,\n",
    "       stimdur,\n",
    "       tr,\n",
    "       outputdir=outputdir_baseline)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_glmsingle}')\n",
    "    \n",
    "    results_assumehrf = dict()\n",
    "    results_assumehrf['typea'] = np.load(join(outputdir_baseline,'TYPEA_ONOFF.npy'),allow_pickle=True).item()\n",
    "    results_assumehrf['typeb'] = np.load(join(outputdir_baseline,'TYPEB_FITHRF.npy'),allow_pickle=True).item()\n",
    "    \n",
    "    # note that even though we are loading TYPEB_FITHRF betas, HRF fitting\n",
    "    # has been turned off and this struct field will thus contain the\n",
    "    # outputs of a GLM fit using the canonical HRF.\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\n",
    "    '\\telapsed time: ',\n",
    "    f'{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，\"假设hrf \"的betas来自我们的基线GLM（关闭了HRF拟合）的 \"typeb \"字段，而 \"fit hrf \"的betas也来自启用了所有默认GLMsingle程序的GLM的 \"typeb \"字段。\n",
    "# create dictionary containing the GLM betas from the four different models we will compare. note that the \"assume hrf\" betas come from the \"typeb\" field of our baseline GLM (with HRF fitting turned off), and that the \"fit hrf\" betas also come from the \"typeb\" field of the GLM that ran with all default GLMsingle routines enabled\n",
    "\n",
    "models = dict()\n",
    "models['assumehrf'] = results_assumehrf['typeb']['betasmd'].reshape(xyz + (750,))\n",
    "models['fithrf'] = results_glmsingle['typeb']['betasmd']\n",
    "models['fithrf_glmdenoise'] = results_glmsingle['typec']['betasmd']\n",
    "models['fithrf_glmdenoise_rr'] = results_glmsingle['typed']['betasmd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取用于可靠性计算的重复条件的指数。 Get indices of repeated conditions to use for reliability calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare the results of different GLMs we are going to calculate the\n",
    "# voxel-wise split-half reliablity for each model. reliability values\n",
    "# reflect a correlation between beta weights for repeated presentations of\n",
    "# the same conditions. in short, we are going to check how\n",
    "# reliable/reproducible are the single trial responses to repeated\n",
    "# conditions estimated with each GLM type.\n",
    "\n",
    "# this NSD scan session has a large number of images that are just shown\n",
    "# once during the session, some images that are shown twice, and a few that\n",
    "# are shown three times. in the code below, we are attempting to locate the\n",
    "# indices in the beta weight GLMsingle outputs modelmd(x,y,z,trials) that\n",
    "# correspond to repeated images. here we only consider stimuli that have\n",
    "# been presented at least twice. for the purpose of the example we ignore\n",
    "# the 3rd repetition of the stimulus.\n",
    "\n",
    "# consolidate design matrices\n",
    "designALL = np.concatenate(design,axis=0)\n",
    "\n",
    "# construct a vector containing 0-indexed condition numbers in chronological order\n",
    "corder = []\n",
    "for p in range(designALL.shape[0]):\n",
    "    if np.any(designALL[p]):\n",
    "        corder.append(np.argwhere(designALL[p])[0,0])\n",
    "        \n",
    "corder = np.array(corder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the first few entries\n",
    "print(corder[:3])\n",
    "\n",
    "# note that [374 496 7] means that the first stimulus trial involved\n",
    "# presentation of the 374th condition (zero-indexed), the second stimulus trial \n",
    "# involved presentation of the 496th condition, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to compute split-half reliability, we have to do some indexing.\n",
    "# we want to find images with least two repetitions and then prepare a\n",
    "# useful matrix of indices that refer to when these occur.\n",
    "\n",
    "repindices = [] # 2 x images containing stimulus trial indices.\n",
    "\n",
    "# the first row refers to the first presentation; the second row refers to\n",
    "# the second presentation.\n",
    "for p in range(designALL.shape[1]): # loop over every condition\n",
    "    \n",
    "    temp = np.argwhere(corder==p)[:,0] # find indices where this condition was shown\n",
    "    \n",
    "    # note that for conditions with 3 presentations, we are simply ignoring the third trial\n",
    "    if len(temp) >= 2:\n",
    "        repindices.append([temp[0], temp[1]]) \n",
    "\n",
    "repindices = np.vstack(np.array(repindices)).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at a few entries\n",
    "print(repindices[:,:3])\n",
    "\n",
    "# notice that the first condition is presented on the 216th zero-indexed \n",
    "# stimulus trial and the 485th stimulus trial, the second condition is presented on the\n",
    "# 217th and 620st stimulus trials, and so on.\n",
    "\n",
    "print(f'there are {repindices.shape[1]} repeated conditions in the experiment')\n",
    "\n",
    "# now, for each voxel we are going to correlate beta weights describing the\n",
    "# response to images presented for the first time with beta weights\n",
    "# describing the response from the repetition of the same image. with 136\n",
    "# repeated conditions, the correlation for each voxel will reflect the\n",
    "# relationship between two vectors with 136 beta weights each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化皮层ROI，定义视觉反应区域  Visualize cortical ROI defining visually-responsive areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask defining liberal visual cortex ROI. \"nsdgeneral\" is a general ROI \n",
    "# that was manually drawn on fsaverage covering voxels responsive to the NSD experiment \n",
    "# in the posterior aspect of cortex. for the sake of simplicity we will focus \n",
    "# on voxels within this ROI in computing split-half reliability\n",
    "\n",
    "nsdgeneral_roi = roi.astype(float)\n",
    "\n",
    "# convert voxels outside ROI to nan for overlay plotting\n",
    "nsdgeneral_roi[nsdgeneral_roi==0] = np.nan \n",
    "\n",
    "# get mean fMRI volume from run 1\n",
    "meanvol = np.squeeze(np.mean(data[0].reshape(xyzt),axis=3))\n",
    "\n",
    "# plot ROI on top of overlay\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(meanvol,cmap='gray')\n",
    "plt.imshow(nsdgeneral_roi,cmap='Blues',clim=(0,2))\n",
    "\n",
    "plt.title('voxels in nsdgeneral ROI')\n",
    "plt.box(False)\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算每个β版本的ROI内的分半可靠性中值  Compute median split-half reliability within the ROI for each beta version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, let's compute split-half reliability. we are going to loop\n",
    "# through our 4 models and calculate split-half reliability for each of them\n",
    "\n",
    "vox_reliabilities = [] # output variable for reliability values\n",
    "\n",
    "modelnames = list(models.keys())\n",
    "\n",
    "# for each beta version...\n",
    "for m in range(len(modelnames)):\n",
    "    \n",
    "    print(f'computing reliability for beta version: {modelnames[m]}')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # get the repeated-condition GLM betas using our repindices variable\n",
    "    betas = models[modelnames[m]][:,:,:,repindices] # automatically reshapes to (X x Y x Z x 2 x nConditions)\n",
    "    x,y,z = betas.shape[:3] \n",
    "    \n",
    "    rels = np.full((x,y,z),np.nan)\n",
    "    \n",
    "    # loop through voxels in the 3D volume...\n",
    "    for xx in tqdm(range(x)):\n",
    "        for yy in range(y):\n",
    "            for zz in range(z):\n",
    "                \n",
    "                # reliability at a given voxel is pearson correlation between response profiles from first and \n",
    "                # second image presentations (dim = 136 conditions)\n",
    "                rels[xx,yy,zz] = np.corrcoef(betas[xx,yy,zz,0],\n",
    "                                             betas[xx,yy,zz,1])[1,0]\n",
    "          \n",
    "    vox_reliabilities.append(rels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估由GLMsingle产生的可靠性变化  Assess change in reliability yielded by GLMsingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each GLM we will calculate median reliability for voxels within the\n",
    "# nsdgeneral visual ROI and compare using a bar graph\n",
    "\n",
    "comparison = []\n",
    "for vr in vox_reliabilities:\n",
    "    comparison.append(np.nanmedian(vr[nsdgeneral_roi==1]))\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(121)\n",
    "plt.bar(np.arange(len(comparison)),comparison,width=0.5)\n",
    "plt.title('Median voxel split-half reliability of GLM models')\n",
    "plt.xticks(np.arange(4),np.array(['ASSUMEHRF', 'FITHRF', 'FITHRF\\nGLMDENOISE', 'FITHRF\\nGLMDENOISE\\nRR']));\n",
    "plt.ylim([0.1,0.2])\n",
    "\n",
    "# draw plot showing the change in reliability between the baseline GLM\n",
    "# and the final output of GLMsingle (fithrf-glmdenoise-RR betas)\n",
    "vox_improvement = np.squeeze(vox_reliabilities[3] - vox_reliabilities[0])\n",
    "vox_improvement[nsdgeneral_roi != 1] = np.nan\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(meanvol,cmap='gray',aspect='auto')\n",
    "plt.imshow(vox_improvement,cmap='RdBu_r',clim=(-0.3,0.3),aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('change in nsdgeneral voxel reliability**\\ndue to GLMsingle (r)')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('\\n**plotting (FITHRF_GLMDENOISE_RR - ASSUMEHRF) reliabilities');\n",
    "\n",
    "# 注意，从第一个版本到第二个版本到第三个版本再到最后的第四个版本的GLM结果，可靠性有了系统性的提高。这些提高分别反映了HRF拟合的增加，数据驱动的滋生回归因子的推导和使用，以及使用脊回归作为规范化紧密间隔的实验试验的不稳定性的方法。根据个人的实验目标，可以通过设置选项标志来激活这些分析特征的子集。\n",
    "\n",
    "# 另外，请记住，在上图中，我们只是把中位数作为中心趋势的一个指标来显示（例如，你可能想在散点图中浏览单个体素）。\n",
    "\n",
    "# notice that there is systematic increase in reliability moving from the first to the second to the third to the final fourth version of the GLM results. these increases reflect, respectively, the addition of HRF fitting, the derivation and use of data-driven nuisance regressors, and the use of ridge regression as a way to regularize the instability of closely spaced experimental trials. depending on one's experimental goals, it is possible with setting of option flags to activate a subset of these analysis features.\n",
    "\n",
    "# also, keep in mind that in the above figure, we are simply showing the median as a metric of the central tendency (you may want to peruse individual voxels in scatter plots, for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}